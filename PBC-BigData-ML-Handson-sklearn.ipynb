{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBC BigData/MLハンズオン\n",
    "- S3上に配置したデータをAthenaを経由してSageMakerに取得する\n",
    "- SageMaker上でデータの可視化やデータ加工を行う\n",
    "- 広く利用されている機械学習ライブラリーである、Scikit-Learnを使ったモデル構築を試してみる\n",
    "- 構築したモデルをSageMakerの推論Endpointとしてデプロイし、推論APIを構築する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"MediumSlateBlue\">1. データを理解する</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python上からAthenaを操作するために便利なライブラリー AWS Data Wranglerを利用する\n",
    "session = awswrangler.Session()\n",
    "\n",
    "# S3に格納されたjson_salesの件数を確認する\n",
    "# この関数はPandas DataFrameを戻すが、表示させるだけの場合は変数に格納する必要はない\n",
    "session.pandas.read_sql_athena(\n",
    "    sql=\"select count(*) as json_sales_count from json_sales\",\n",
    "    database=\"workshop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 少量のデータを取得してJupyter上で見てみる\n",
    "session.pandas.read_sql_athena(\n",
    "    sql=\"select * from json_sales where prod_id is not null limit 5\",\n",
    "    database=\"workshop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後続処理で利用するデータは変数に格納する\n",
    "# ここでは全期間のamaount_soldを日ごとに合計する処理をAthenaで行い、サマリーした結果をJupyter上で保持する\n",
    "df_sales_daily = session.pandas.read_sql_athena(\n",
    "    sql=\"select time_id, sum(amount_sold) as daily_sum from json_sales \\\n",
    "         where prod_id is not null \\\n",
    "         group by time_id order by time_id\",\n",
    "    database=\"workshop\"\n",
    ")\n",
    "print('取得したデータの件数（日数）: {}'.format(df_sales_daily.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_daily.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 月単位の売り上げを集計してグラフ化する\n",
    "df_sales_daily['time_id'] = pd.to_datetime(df_sales_daily.time_id)\n",
    "df_sales_daily.set_index('time_id', inplace=True)\n",
    "df_sales_monthly = df_sales_daily.resample('M').sum()\n",
    "\n",
    "df_sales_monthly.rename(columns={'daily_sum':'monthly_sum'}, inplace=True)\n",
    "df_sales_monthly.plot(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=\"MediumSlateBlue\">2. データを加工する</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 過去の実績値の推移を入力データにするため、1ヶ月前、2ヶ月前、3ヶ月前、12ヶ月前の実績値を取得する\n",
    "df_data = df_sales_monthly['2007-01-01':].copy()\n",
    "df_data['monthly_sum'] = df_data.monthly_sum / 1000000\n",
    "df_data['1month_ago'] = df_data.monthly_sum.shift(1)\n",
    "df_data['2month_ago'] = df_data.monthly_sum.shift(2)\n",
    "df_data['3month_ago'] = df_data.monthly_sum.shift(3)\n",
    "df_data['12month_ago'] = df_data.monthly_sum.shift(12)\n",
    "\n",
    "# 過去のデータをシフトしているため、期間の開始から12レコード分は欠損値が発生する\n",
    "df_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損値が発生した部分のデータは削除する\n",
    "df_data = df_data['2008-01-01':].copy()\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=\"MediumSlateBlue\">3. モデルを構築する</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# モデルのトレーニングを行う鑵子を定義する\n",
    "def train_lr_model(feature_cols, target_col, train_test_split):\n",
    "    # 学習データとテストデータを分離\n",
    "    train_x = df_data[:train_test_split][feature_cols].values\n",
    "    train_y = df_data[:train_test_split][[target_col]].values\n",
    "    train_index = df_data[:train_test_split].index\n",
    "    test_x = df_data[train_test_split:][feature_cols].values\n",
    "    test_y = df_data[train_test_split:][[target_col]].values\n",
    "    test_index = df_data[train_test_split:].index\n",
    "\n",
    "    # モデルのトレーニングを実行\n",
    "    lr_model = LinearRegression(normalize=True)\n",
    "    lr_model.fit(train_x, train_y)\n",
    "\n",
    "    # テストデータに対して予測を実行\n",
    "    test_pred = lr_model.predict(test_x)\n",
    "\n",
    "    # 実績データと予測結果を結合して返却\n",
    "    df_test = pd.DataFrame({'label': test_y[:,0], \n",
    "                            'pred': test_pred[:,0]}, index=test_index)\n",
    "    df_train = pd.DataFrame({'label': train_y[:,0], \n",
    "                           }, index=train_index)\n",
    "    df_result = pd.concat([df_train, df_test], sort=False).sort_index()\n",
    "\n",
    "    return lr_model, df_test, df_result\n",
    "\n",
    "# 1回目のモデル作成試行\n",
    "feature_cols1 = ['1month_ago', '2month_ago', '3month_ago', '12month_ago']\n",
    "target_col = 'monthly_sum'\n",
    "train_test_split = '2011-12-31'\n",
    "\n",
    "model1, df_test1, df_result1 = train_lr_model(feature_cols1, target_col, train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('r2', r2_score(df_test1.label, df_test1.pred))\n",
    "df_result1.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル1の予測傾向\n",
    "- r2乗値は約0.76となった\n",
    "- 2012年の予測はある程度実績に追随している\n",
    "- 2013年は、2012年までの増加傾向を反映して上振れした予測値となっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=\"MediumSlateBlue\">3-2. モデルの改善を試みる</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 年々増えていたり、月ごとに周期的な動きをしている傾向を取り込みたい\n",
    "# そのために特徴量を追加する\n",
    "\n",
    "df_data.reset_index(inplace=True)\n",
    "\n",
    "df_data['month'] = df_data.time_id.dt.month\n",
    "starting_year = df_data.time_id.dt.year.min()\n",
    "df_data['year_delta'] = df_data.time_id.dt.year - starting_year\n",
    "df_data.set_index('time_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# もう一度モデルの学習を実行\n",
    "feature_cols2 = ['1month_ago', '2month_ago', '3month_ago', '12month_ago', 'month', 'year_delta']\n",
    "model2, df_test2, df_result2 = train_lr_model(feature_cols2, target_col, train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('r2', r2_score(df_test2.label, df_test2.pred))\n",
    "df_result2.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル2の予測傾向\n",
    "- r2乗値は約0.84に増加して、全体としての予実差は改善された\n",
    "- 2013年の上振れ傾向は是正されているが、2012年がやや下振れした予測となった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルはどんなパラメーターが算出されたのか\n",
    "def WriteCoef(model, feature_cols):\n",
    "    [print('coefficient[', v, ']=', model.coef_[0][i]) for i, v in enumerate(feature_cols)]\n",
    "    print('intercept = ', model.intercept_)\n",
    "    \n",
    "print('model1:')\n",
    "WriteCoef(model1, feature_cols1)\n",
    "print()\n",
    "print('model2:')\n",
    "WriteCoef(model2, feature_cols2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=\"MediumSlateBlue\">4. モデルを推論用に展開する</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Sagemaker session object\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Sagemakerの実行ロールを取得\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3上のデータ出力先を定義\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'sagemaker-handson'\n",
    "s3_path = 's3://{}/{}/monthly_sum'.format(default_bucket, s3_prefix)\n",
    "\n",
    "# 事前に出力先のS3パスを掃除しておく\n",
    "session.s3.delete_objects(path=s3_path)\n",
    "\n",
    "out_cols = ['monthly_sum', '1month_ago', '2month_ago', '3month_ago', '12month_ago', 'month', 'year_delta']\n",
    "df_train_estimator = df_data[:train_test_split][out_cols]\n",
    "df_test_estimator = df_data[train_test_split:][out_cols]\n",
    "\n",
    "# Jupyter上で処理していたデータをS3にdumpする\n",
    "data_on_s3 = session.pandas.to_csv(dataframe=df_train_estimator, path=s3_path, preserve_index=False )\n",
    "data_on_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3上のデータ出力先を定義\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'sagemaker-handson'\n",
    "s3_path = 's3://{}/{}/monthly_sum'.format(default_bucket, s3_prefix)\n",
    "\n",
    "# 事前に出力先のS3パスを掃除しておく\n",
    "session.s3.delete_objects(path=s3_path)\n",
    "\n",
    "out_cols = ['monthly_sum', '1month_ago', '2month_ago', '3month_ago', '12month_ago', 'month', 'year_delta']\n",
    "df_train_estimator = df_data[:train_test_split][out_cols]\n",
    "df_test_estimator = df_data[train_test_split:][out_cols]\n",
    "\n",
    "# Jupyter上で処理していたデータをS3にdumpする\n",
    "data_on_s3 = session.pandas.to_csv(dataframe=df_data, path=s3_path, preserve_index=False )\n",
    "data_on_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'sklearn_monthly_sum.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"local\",\n",
    "    role=role,\n",
    "    hyperparameters={'normalize': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【参考】大量のデータを使用する重い学習に、学習用の別インスタンスを利用する場合は以下のように記述する\n",
    "```python\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'sklearn_monthly_sum.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={'normalize': True})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# S3に出力したデータを指定してモデルを学習させる\n",
    "sklearn.fit({'train': s3_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習が完了したので予測のためのpredictorを作成する\n",
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【参考】実運用に利用するEndpointをdeployする際はインスタンスタイプの記述を変更する\n",
    "```python\n",
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color=\"MediumSlateBlue\">5. endpointを使用してpredictionを行う</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_estimator.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimatorに合わせたテストデータを作成する\n",
    "test_x = df_test_estimator.values[:,1:]\n",
    "test_y = df_test_estimator.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predictor.predict(test_x)\n",
    "df_test_result = pd.DataFrame({'label':test_y, 'pred':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_result.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
